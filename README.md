Download Link: https://assignmentchef.com/product/solved-csc401-assignment-3-speaker-identification-and-speech-recognition
<br>
This assignment introduces you to Gaussian mixture modelling, and two basic tasks in speech technology: <em>speaker identification</em>, in which we try to determine <em>who </em>is talking, and <em>speech recognition</em>, in which we try to determine <em>what </em>was said.

The assignment is divided into two sections. In the first, you will experiment with speaker identification by training mixtures of Gaussians to the acoustic characteristics of individual speakers, and then identify speakers based on these models. In the second section, you will evaluate two speech recognition engines.

The data come from the <strong><a href="https://catalog.ldc.upenn.edu/LDC2013S09">CSC Deceptive Speech</a> </strong>corpus, which was developed by Columbia University, SRI International, and University of Colorado Boulder. It consists of 32 hours of audio interview from 32 native speakers of Standard American English (16 male,16 female) recruited from the Columbia University student population and the community. The purpose of the study was to distinguish deceptive speech from non-deceptive speech using machine learning techniques on extracted features from the corpus.

Data are in /u/cs401/A3/data/; each sub-folder represents speech from one speaker and contains raw audio, pre-computed MFCCs, and orthographic transcripts. Further file descriptions are in Appendix A.

<h1>1           Speaker Identification</h1>

Speaker identification is the task of correctly identifying speaker <em>s<sub>c </sub></em>from among <em>S </em>possible speakers <em>s<sub>i</sub></em><sub>=1<em>..S </em></sub>given an input speech sequence <em>X</em>, consisting of a succession of <em>d</em>-dimensional real vectors. In the interests of efficiency, <em>d </em>= 13 in this assignment. Each vector represents a small 25 ms unit of speech called a <em>frame</em>. Speakers are identified by training data that are ascribed to them. This is a discrete classification task (choosing among several speakers) that uses continuous-valued data (the vectors of real numbers) as input.

<h2>Gaussian Mixture Models</h2>

<em>Gaussian mixture models </em>are often used to generalize models from sparse data. They can tightly constrain large-dimensional data by using a small number of components but can, with many more components, model arbitrary density distributions. Sometimes, they are simply used because the domain being modelled appears to have multiple modes.

Given <em>M </em>components, GMMs are modelled by a collection of parameters, <em>θ </em>= {<em>ω<sub>m</sub></em><sub>=1<em>..M</em></sub><em>,µ<sub>m</sub></em><sub>=1<em>..M</em></sub><em>,</em>Σ<em><sub>m</sub></em><sub>=1<em>..M</em></sub>}, where <em>ω<sub>m </sub></em>is the probability that an observation is generated by the <em>m<sup>th </sup></em>component. These are subject

<sup>0</sup>Copyright    2019, Frank Rudzicz. All rights reserved.

to the constraint that <sup>P</sup><em><sub>m </sub>ω<sub>m </sub></em>= 1 and 0 ≤ <em>ω<sub>m </sub></em>≤ 1. Each component is a multivariate Gaussian distribution, which is characterized by that component’s mean, <em>µ<sub>m</sub></em>, and covariance matrix, Σ<em><sub>m</sub></em>. For reasons of computational efficiency, we will reintroduce some independence assumptions by assuming that every component’s covariance matrix is diagonal, i.e.:

<table width="0">

 <tbody>

  <tr>

   <td width="110"><sup> </sup>Σ<em><sub>m</sub></em>[1] 0Σ<em><sub>m </sub></em>=  …0</td>

   <td width="50">0Σ<em><sub>m</sub></em>[2]0</td>

   <td width="33">·········</td>

   <td width="56">0 0 Σ<em><sub>m</sub></em>[<em>d</em>]</td>

  </tr>

 </tbody>

</table>

for some vector Σ<em><sup>~ </sup><sub>m</sub></em>. Therefore, only <em>d </em>parameters are necessary to characterize a component’s (co)variance.

<h2>1.1            Utility functions</h2>

First, we implement three utility functions in /u/cs401/A3/code/a3 gmm.py. First, implement log b m x, which implements the log observation probability of <em>x<sub>t </sub></em>for the <em>m<sup>th </sup></em>mixture component, i.e., the log of:

(1)

Next, implement log p m x, which is the log probability of <em>m </em>given <em>x<sub>t </sub></em>using model <em>θ</em>, i.e., the log of:

(2)

Finally, implement logLik, which is the log likelihood of a set of data <em>X</em>, i.e.:

)                                                                        (3)

where

)                                                                                 (4)

and <em>b<sub>m </sub></em>is defined in Equation 1. For efficiency, we just pass <em>θ </em>and precomputed <em>b<sub>m</sub></em>(<em>x~<sub>t</sub></em>) to this function.

<h2>1.2            Training Gaussian mixture models</h2>

Now we train an <em>M</em>-component GMM for each of the speakers in the data set. Specifically, for each speaker <em>s</em>, train the parameters <em>θ<sub>s </sub></em>= {<em>ω<sub>m</sub></em><sub>=1<em>..M</em></sub><em>,µ<sub>m</sub></em><sub>=1<em>..M</em></sub><em>,</em>Σ<em><sub>m</sub></em><sub>=1<em>..M</em></sub>} according to the method described in Appendix B. In all cases, assume that covariance matrices Σ<em><sub>m </sub></em>are diagonal. Start with <em>M </em>= 8. You’ll be asked to experiment with that in Section 2.4. Complete the function train in /u/cs401/A3/code/a3 gmm.py.

<h2>1.3            Classification with Gaussian mixture models</h2>

Now we test each of the test sequences we’ve already set aside for you in the main function. I.e., we check if the <em>actual </em>speaker is also the most likely speaker, ˆ<em>s</em>:

(5)

<em>s</em>=1

Complete the function test in /u/cs401/A3/code/a3 gmm.py. Run through a train-test cycle, and save the output that this function writes to stdout, using the <em>k </em>= 5 top alternatives, to the file gmmLiks.txt.

<h2>1.4            Experiments and discussion</h2>

Experiment with the settings of <em>M </em>and  if you wish). For example, what happens to classification accuracy as the number of components decreases? What about when the number of possible speakers, <em>S</em>, decreases? You will be marked on the detail with which you empirically answer these questions and whether you can devise one or more additional valid experiments of this type.

Additionally, your report should include short hypothetical answers to the following questions:

<ul>

 <li>How might you improve the classification accuracy of the Gaussian mixtures, without adding more training data?</li>

 <li>When would your classifier decide that a given test utterance comes from none of the trained speaker models, and how would your classifier come to this decision?</li>

 <li>Can you think of some alternative methods for doing speaker identification that don’t use Gaussian mixtures?</li>

</ul>

Put your experimental analysis and answers to these questions in the file gmmDiscussion.txt.

<h1>2           Speech Recognition</h1>

Automatic speech recognition (ASR) is the task of correctly identifying a word sequence given an input speech sequence <em>X</em>. To simplify your lives, we have ran two popular ASR engines on our data: the opensource and highly customizable <strong><a href="https://kaldi-asr.org">Kaldi</a> </strong>(specifically, a bi-directional LSTM <u><a href="https://kaldi-asr.org/models.html">model</a> </u>trained on the <u><a href="https://catalog.ldc.upenn.edu/LDC2004S13">Fisher </a></u>corpus), and the neither-open-source-nor-particularly-customizable <strong><a href="https://cloud.google.com/speech/">Google Speech API</a></strong><a href="https://cloud.google.com/speech/">.</a>

We want to see which of Kaldi and Google are the most accurate on our data. For each speaker in our data, we have three transcript files: transcripts.txt (the gold-standard transcripts, from humans), transcripts.Kaldi.txt (the ASR output of Kaldi), and transcripts.Google.txt (the ASR output of Google); see Appendix A.

Complete the file at /u/cs401/A3/code/a3 levenshtein.py. Specifically, in the Levenshtein function, accept lists of words <em>r </em>(Reference) and <em>h </em>(hypothesis), and return a 4-item list containing the floatingpoint WER, the number of substitutions, the number of insertions, and the number of deletions where

Assume that the cost of a substitution is 0 if the words are identical and 1 otherwise. The costs of insertion and deletion are both 1.

In the main function, iterate through each of the speakers, and iterate through each line <em>i </em>of their transcripts. For each line, preprocess these transcripts by removing all punctuation (other than [ and ]) and setting the text to lowercase. Output the following to stdout:

[SPEAKER] [SYSTEM] [i] [WER] S:[numSubstitutions], I:[numInsertions], D:[numDeletions]

where [SYSTEM] is either ‘Kaldi’ or ‘Google’.

Save this output and put it into asrDiscussion.txt.

On the second-to-last line of asrDiscussion.txt, in free text, summarize your findings by reporting the average and standard deviation of WER for each of Kaldi and Google, separately, over all of these lines. If you want to be fancy, you can compute a statistical test of significance to see if one is better than the other, but you don’t need to.

On the last line of asrDiscussion.txt, add a sentence or two describing anything you observe about the types of errors being made by each system, by manually examining the transcript files.

<h1>3           Bonus</h1>

We will give up to 10 bonus marks for innovative work going substantially beyond the minimal requirements. These marks can make up for marks lost in other sections of the assignment, but your overall mark for this assignment cannot exceed 100%. You may decide to pursue any number of tasks of your own design related to this assignment, although you should consult with the instructor or the TA before embarking on such exploration. Certainly, the rest of the assignment takes higher priority. Some ideas:

<h2>Voice banking</h2>

We are running a large study or ‘normative’ data in which people from the general population donate their speech (and language) data so that we can learn subtle differences in pathological populations. If you go to <a href="https://www.cs.toronto.edu/talk2me/">https://www.cs.toronto.edu/talk2me/</a><a href="https://www.cs.toronto.edu/talk2me/">,</a> you can obtain 5 bonus points if you complete 10 sessions, each at least 1 day apart. There is no limit on your age, or first language. Currently, only the Chrome and most recent Firefox browser are supported. Create a new username for this assignment, and indicate your username in your submission so that we can validate your submitted data.

<h2>Dimensionality reduction</h2>

Principal components analysis (PCA) is a method that converts some multivariate representation of data into a lower-dimensional representation by transforming the original data according to mutually orthogonal principal components.

Implement an algorithm that discovers a <em>d</em>×<em>d</em><sup>0 </sup>matrix <em>W </em>that transforms a <em>d</em>-dimensional vector, <em>~x </em>into a <em>d</em><sup>0</sup>-dimensional vector <em>~y </em>through a linear transformation based on PCA, where <em>d</em><sup>0 </sup><em>&lt; d</em>. Repeat speaker identification using data that has been transformed by PCA and report on what you observe, e.g., for different values of <em>d</em><sup>0</sup>. Submit all code and materials necessary to repeat your experiments.

<h2>ASR with sequence-to-sequence models</h2>

Try to do better than Kaldi or Google by implementing:

Chiu C-C, Sainath TN, Wu Y, <em>et al. </em>(2017) State-of-the-art Speech Recognition With Sequence-toSequence Models. <a href="https://arxiv.org/abs/1712.01769">http://arxiv.org/abs/1712.01769.</a>

Consider using open-source end-to-end ASR using TensorFlow, e.g., <a href="https://github.com/fordDeepDSP/deepSpeech">deepSpeech.</a>

<h2>Truth-and-lie detection</h2>

Each of the utterances has been labelled as either truthful or deceitful (see Appendix A). Train and test models to tell these utterances apart using the provided data. E.g.,

<ul>

 <li>Train a GMM for each of the Truth and Lie categories, using your code from Section 2.</li>

 <li>Try <strong><a href="https://www.tensorflow.org/tutorials/recurrent">recurrent neural networks</a> </strong>that read one MFCC vector at a time.</li>

 <li>Extract engineered features, such as those extracted in Assignment 1, from the text transcripts and classify using discriminative models in <strong><a href="http://scikit-learn.org/stable/">scikit-learn</a></strong><a href="http://scikit-learn.org/stable/">.</a> Are words more discriminative than the audio?</li>

</ul>

Consider how errors in ASR transcripts affect those extracted features and therefore overall system accuracy in a manner not dissimilar to Zhou L, Fraser KC, Rudzicz F. (2016) <u>S</u><a href="http://www.cs.toronto.edu/~frank/Download/Papers/IS16_ASRAD.pdf">peech recognition in </a><a href="http://www.cs.toronto.edu/~frank/Download/Papers/IS16_ASRAD.pdf">Alzheimer’s disease and in its assessment.</a> In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH